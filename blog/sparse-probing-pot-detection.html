<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Sparse Linear Probing for Efficient Detection | Nick McCarty</title>
  <meta name="description" content="Using L1-regularized linear probes to identify minimal feature subsets from SAM and Faster R-CNN that are sufficient for pot detection.">
  <link rel="icon" type="image/png" sizes="32x32" href="../assets/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../assets/images/favicon-16x16.png">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <script src="../assets/js/color-modes.js"></script>
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-YT69KZ91W7"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-YT69KZ91W7');
  </script>
</head>
<body>
  <nav>
    <a href="../" class="logo" aria-label="Home"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16"><path d="M8.354 1.146a.5.5 0 0 0-.708 0l-6 6A.5.5 0 0 0 1.5 7.5v7a.5.5 0 0 0 .5.5h4.5a.5.5 0 0 0 .5-.5v-4h2v4a.5.5 0 0 0 .5.5H14a.5.5 0 0 0 .5-.5v-7a.5.5 0 0 0-.146-.354L13 5.793V2.5a.5.5 0 0 0-.5-.5h-1a.5.5 0 0 0-.5.5v1.293zM2.5 14V7.707l5.5-5.5 5.5 5.5V14H10v-4a.5.5 0 0 0-.5-.5h-3a.5.5 0 0 0-.5.5v4z"/></svg></a>
    <div class="nav-right">
      <div class="nav-links">
        <a href="../about.html">About</a>
        <a href="../blog.html" class="active">Blog</a>
        <a href="../contact.html">Contact</a>
      </div>
      <button class="theme-toggle" aria-label="Toggle theme">
        <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M8 11a3 3 0 1 1 0-6 3 3 0 0 1 0 6m0 1a4 4 0 1 0 0-8 4 4 0 0 0 0 8M8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0m0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13m8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5M3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8m10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0m-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0m9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707M4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708"/>
        </svg>
        <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M6 .278a.77.77 0 0 1 .08.858 7.2 7.2 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277q.792-.001 1.533-.16a.79.79 0 0 1 .81.316.73.73 0 0 1-.031.893A8.35 8.35 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.75.75 0 0 1 6 .278M4.858 1.311A7.27 7.27 0 0 0 1.025 7.71c0 4.02 3.279 7.276 7.319 7.276a7.32 7.32 0 0 0 5.205-2.162q-.506.063-1.029.063c-4.61 0-8.343-3.714-8.343-8.29 0-1.167.242-2.278.681-3.286"/>
          <path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.73 1.73 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.73 1.73 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.73 1.73 0 0 0 1.097-1.097zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.16 1.16 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.16 1.16 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732z"/>
        </svg>
      </button>
    </div>
  </nav>

  <main>
    <a href="../blog.html" class="back-link">&larr; Back to Blog</a>

    <article>
      <h1>Sparse Linear Probing for Efficient Detection</h1>
      <div class="post-meta">January 9, 2026 &bull; 10 min read</div>

      <p>
        Vision models like SAM have 1024 backbone channels; Faster R-CNN's ResNet has 2048. But how many of
        these features actually matter for a specific task? Sparse linear probing answers this question by
        training L1-regularized classifiers that naturally select minimal feature subsets. The results have
        practical implications for both interpretability and efficient deployment.
      </p>

      <h2>The Sparse Probing Setup</h2>

      <p>
        <span class="glossary-term" data-term="linear-probing">Linear probing</span> trains a simple classifier on frozen features. We extract backbone activations, flatten
        them to per-pixel feature vectors, and train logistic regression to predict pot vs. background. The
        key: <span class="glossary-term" data-term="l1-regularization">L1 regularization</span> drives most coefficients to exactly zero, leaving only the channels that
        genuinely contribute to the decision.
      </p>

      <pre><code>from sklearn.linear_model import LogisticRegression

# L1-regularized logistic regression
# C controls regularization: smaller C = more sparsity
model = LogisticRegression(
    penalty='l1',
    C=0.1,           # Inverse regularization strength
    solver='liblinear',
    class_weight='balanced'  # Handle class imbalance
)

model.fit(X_train_scaled, y_train)

# Count non-zero coefficients
n_active = np.sum(np.abs(model.coef_[0]) > 1e-5)
print(f"Active features: {n_active} / {C} ({n_active/C*100:.1f}%)")</code></pre>

      <p>
        The regularization strength C controls the sparsity-accuracy tradeoff. Smaller C means stronger
        regularization and more zero coefficients. We sweep across C values to find the sweet spot.
      </p>

      <h2>Data Preparation</h2>

      <p>
        Training a sparse probe requires aligned features and labels. We use SAM's segmentation masks as
        ground truth&mdash;downsampled to match feature resolution. Balanced sampling ensures roughly
        equal positive (pot) and negative (background) pixels despite extreme <span class="glossary-term" data-term="class-imbalance">class imbalance</span> in the raw data.
      </p>

      <pre><code># Downsample mask to feature resolution
mask_downsampled = zoom(full_mask, 1/DOWNSAMPLE_FACTOR, order=0)

# Reshape features: (C, H, W) -> (H*W, C)
X = features.reshape(C, -1).T
y = mask_downsampled.ravel()

# Balanced sampling (50/50 despite ~5% pot coverage)
pos_idx = np.where(y == 1)[0]
neg_idx = np.where(y == 0)[0]

n_sample = min(len(pos_idx), len(neg_idx), 250000)
sample_idx = np.concatenate([
    np.random.choice(pos_idx, n_sample, replace=False),
    np.random.choice(neg_idx, n_sample, replace=False)
])</code></pre>

      <p>
        <span class="glossary-term" data-term="feature-standardization">Feature standardization</span> is critical for L1 regularization to work correctly. Without zero mean and
        unit variance, the penalty affects channels unequally.
      </p>

      <h2>Sparsity-Accuracy Tradeoff</h2>

      <p>
        Sweeping across regularization strengths reveals a striking pattern: a small subset of channels
        achieves nearly full performance. In our experiments with SAM's 1024-channel backbone:
      </p>

      <pre><code>C=0.001: 12 active features,   F1=0.82
C=0.01:  47 active features,   F1=0.89
C=0.1:   142 active features,  F1=0.93
C=1.0:   384 active features,  F1=0.94
C=10.0:  621 active features,  F1=0.94</code></pre>

      <p>
        The first ~50 features capture 95% of the performance. Adding hundreds more provides diminishing
        returns. This suggests the model encodes pot detection in a sparse, interpretable subspace rather
        than distributing it uniformly across all channels.
      </p>

      <p>
        We measure performance using <span class="glossary-term" data-term="f1-score">F1 score</span>, which balances precision and recall&mdash;critical when
        both false positives and false negatives have real costs.
      </p>

      <h2>Identifying Critical Channels</h2>

      <p>
        The coefficient magnitudes rank channel importance. Positive coefficients indicate pot-selective
        channels (higher activation = more likely pot); negative coefficients indicate background-selective
        channels.
      </p>

      <pre><code># Rank channels by absolute coefficient
coef_abs = np.abs(model.coef_[0])
feature_importance = pd.DataFrame({
    'channel': np.arange(len(coef_abs)),
    'coefficient': model.coef_[0],
    'rank': np.argsort(np.argsort(-coef_abs)) + 1
})

# Top 10 most important
print(feature_importance.nlargest(10, 'abs_coefficient'))</code></pre>

      <p>
        Visualizing the top channels reveals what they encode. In SAM's <span class="glossary-term" data-term="vit-backbone">ViT backbone</span>, top channels often
        show clear circular activation patterns corresponding to pot locations. Background-selective
        channels activate on soil, shadows, and vegetation.
      </p>

      <h2>Comparing Sparse vs. Full Models</h2>

      <p>
        To validate the sparse selection, we train L2-regularized probes on different channel subsets:
        top 5, 10, 20, 50, 100, and all channels. The comparison confirms that most performance comes
        from the top channels.
      </p>

      <pre><code>Feature subset comparison (test set):
  Top 5:    Precision=0.78  Recall=0.81  F1=0.79
  Top 10:   Precision=0.84  Recall=0.86  F1=0.85
  Top 20:   Precision=0.89  Recall=0.90  F1=0.89
  Top 50:   Precision=0.92  Recall=0.93  F1=0.92
  Top 100:  Precision=0.93  Recall=0.94  F1=0.93
  All 1024: Precision=0.94  Recall=0.95  F1=0.94</code></pre>

      <p>
        Using just 50 channels (5% of total) achieves 98% of full-model F1. The accuracy gap is small;
        the computational savings are substantial.
      </p>

      <h2>Deployment Implications</h2>

      <p>
        Sparse probing has practical deployment value. If only 50 channels matter, we can:
      </p>

      <ul>
        <li><strong>Reduce memory:</strong> Store and process only critical channels (20x reduction)</li>
        <li><strong>Speed inference:</strong> Fewer channels = faster downstream processing</li>
        <li><strong>Simplify models:</strong> A 50-feature linear classifier is trivially deployable</li>
        <li><strong>Enable edge deployment:</strong> Lightweight models fit on resource-constrained devices</li>
      </ul>

      <p>
        The sparse probe itself can serve as a fast detector. Extract the 50 critical channels, apply the
        learned linear weights, and threshold the output. This runs orders of magnitude faster than full
        model inference.
      </p>

      <pre><code># Efficient sparse inference
sparse_channels = feature_importance.head(50)['channel'].values
sparse_features = features[sparse_channels]  # (50, H, W)

# Apply learned weights
weights = model.coef_[0][sparse_channels]
scores = np.tensordot(weights, sparse_features, axes=1) + model.intercept_
predictions = (scores > 0).astype(int)</code></pre>

      <h2>Interpretability Value</h2>

      <p>
        Beyond deployment, sparse probing advances interpretability. The selected channels represent the
        model's "pot detection circuit"&mdash;the minimal computation required for the task. We can
        visualize these channels, study their activation patterns, and understand what visual features
        drive detection.
      </p>

      <p>
        The coefficient signs reveal semantic roles. Channels with positive coefficients detect pot-like
        features (dark circles, regular shapes, specific textures). Channels with negative coefficients
        detect background features that rule out pots (irregular edges, vegetation patterns, shadow
        gradients).
      </p>

      <h2>Cross-Model Comparison</h2>

      <p>
        Running sparse probing on both SAM and Faster R-CNN reveals architectural differences. SAM's ViT
        backbone concentrates pot detection in fewer channels (~50 for 95% performance) compared to
        ResNet (~80 channels). The transformer's global attention may create more efficient representations.
      </p>

      <p>
        The selected channels differ between architectures. Some concepts (edges, shapes) appear in both;
        others are architecture-specific. This suggests multiple valid solutions to the same detection
        problem&mdash;different ways to encode "pot-ness" in feature space.
      </p>

      <h2>Limitations and Extensions</h2>

      <p>
        Sparse probing assumes features are already computed. The probe only measures discriminative value
        for a fixed representation&mdash;it doesn't account for feature computation cost in the backbone.
        A channel might be cheap in linear probing but expensive to compute in the network.
      </p>

      <p>
        The method also assumes linear separability. Non-linear relationships between channels may matter
        for detection but won't be captured by a linear probe. Extensions like polynomial features or
        shallow MLPs can probe for these interactions.
      </p>

      <p>
        Future work: combine sparse probing with circuit extraction to understand not just which channels
        matter but how they interact to produce detections.
      </p>

      <div class="related-post">
        <span class="related-label">Related</span>
        <a href="./extracting-vision-features.html">
          <h4>Extracting Features from Vision Model Backbones</h4>
          <p>How to extract the multi-channel features that sparse probing operates on.</p>
          <span class="read-more">Read more &rarr;</span>
        </a>
      </div>

      <div class="related-post">
        <span class="related-label">Related</span>
        <a href="./circuit-extraction-pot-detection.html">
          <h4>Circuit Extraction: Interpreting Object Detectors</h4>
          <p>Beyond sparse probing: using activation patching to map the full computational circuit.</p>
          <span class="read-more">Read more &rarr;</span>
        </a>
      </div>
    </article>
  </main>

  <!-- Glossary Modal -->
  <div class="modal-overlay glossary-modal" id="glossary-modal">
    <div class="modal">
      <div class="modal-header">
        <h3 id="glossary-term-title"></h3>
        <button class="modal-close" aria-label="Close">&times;</button>
      </div>
      <div class="modal-body">
        <p id="glossary-term-definition"></p>
        <p class="context-note" id="glossary-term-context"></p>
      </div>
    </div>
  </div>

  <script>
    // Glossary term definitions
    const glossaryTerms = {
      'linear-probing': {
        title: 'Linear Probing',
        definition: 'A technique for evaluating neural network representations by training a simple linear classifier on frozen (non-updated) features. If a linear model can solve a task using the features, those features encode task-relevant information in a linearly separable way.',
        context: 'We use linear probing to test which backbone channels contain pot-detection information. A logistic regression trained on frozen SAM/ResNet features tells us which channels are discriminative without modifying the original model.'
      },
      'l1-regularization': {
        title: 'L1 Regularization',
        definition: 'A penalty term added to model training that equals the sum of absolute coefficient values. Unlike L2 (squared values), L1 drives coefficients to exactly zero, producing sparse models that use only a subset of features.',
        context: 'L1 regularization is the key to sparse probing—it forces the classifier to select only the most important channels. Channels that don\'t contribute get exactly zero weight, revealing the minimal feature set needed for detection.'
      },
      'class-imbalance': {
        title: 'Class Imbalance',
        definition: 'When training data has unequal class frequencies. In detection tasks, background pixels vastly outnumber object pixels (often 95%+ background), which can bias models toward always predicting the majority class.',
        context: 'In our orthomosaics, pots cover only ~5% of pixels. Without balanced sampling, a classifier could achieve 95% accuracy by always predicting "background." We sample equal positive/negative pixels to force the model to learn discriminative features.'
      },
      'feature-standardization': {
        title: 'Feature Standardization',
        definition: 'Transforming features to have zero mean and unit variance (z-score normalization). This ensures all features are on the same scale, which is critical for regularization methods that penalize coefficient magnitude.',
        context: 'Without standardization, L1 regularization would penalize high-magnitude features more harshly regardless of their importance. Standardizing ensures the sparsity penalty is applied fairly across all channels.'
      },
      'vit-backbone': {
        title: 'ViT Backbone',
        definition: 'Vision Transformer (ViT) is an architecture that applies transformer self-attention to image patches. Unlike CNNs that use local convolutions, ViT processes global relationships between all image regions, enabling different representational properties.',
        context: 'SAM uses a ViT backbone, which concentrates pot detection in fewer channels (~50) compared to ResNet (~80). The transformer\'s global attention may create more efficient, less redundant representations.'
      },
      'f1-score': {
        title: 'F1 Score',
        definition: 'The harmonic mean of precision and recall: F1 = 2 × (precision × recall) / (precision + recall). It balances both metrics, ranging from 0 to 1, where 1 is perfect. Useful when both false positives and false negatives matter.',
        context: 'F1 is our primary metric because pot detection has costs on both sides: false positives waste inspection time, false negatives miss plants. F1 captures this tradeoff better than accuracy alone.'
      }
    };

    // Modal functionality
    const modal = document.getElementById('glossary-modal');
    const titleEl = document.getElementById('glossary-term-title');
    const definitionEl = document.getElementById('glossary-term-definition');
    const contextEl = document.getElementById('glossary-term-context');
    const closeBtn = modal.querySelector('.modal-close');

    // Open modal on term click
    document.querySelectorAll('.glossary-term').forEach(term => {
      term.addEventListener('click', () => {
        const termId = term.dataset.term;
        const termData = glossaryTerms[termId];

        if (termData) {
          titleEl.textContent = termData.title;
          definitionEl.textContent = termData.definition;
          contextEl.textContent = termData.context;
          modal.classList.add('active');
        }
      });
    });

    // Close modal
    closeBtn.addEventListener('click', () => modal.classList.remove('active'));
    modal.addEventListener('click', (e) => {
      if (e.target === modal) modal.classList.remove('active');
    });
    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape') modal.classList.remove('active');
    });
  </script>

  <footer>
    <div class="social-links">
      <a href="https://linkedin.com/in/nicholasmccarty" target="_blank" rel="noopener" aria-label="LinkedIn">
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401m-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4"/>
        </svg>
      </a>
      <a href="https://github.com/nickmccarty" target="_blank" rel="noopener" aria-label="GitHub">
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8"/>
        </svg>
      </a>
      <a href="mailto:nick@upskilled.consulting" aria-label="Email">
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1zm13 2.383-4.708 2.825L15 11.105zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741M1 11.105l4.708-2.897L1 5.383z"/>
        </svg>
      </a>
    </div>
    <p>&copy; 2025 Nick McCarty. All rights reserved.</p>
  </footer>
</body>
</html>
