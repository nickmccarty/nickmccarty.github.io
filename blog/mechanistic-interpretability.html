<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Mechanistic Interpretability for Agricultural AI | Nick McCarty</title>
  <meta name="description" content="Exploring how mechanistic interpretability techniques can help us understand what vision models learn about agricultural environments.">
  <link rel="icon" type="image/png" sizes="32x32" href="../assets/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../assets/images/favicon-16x16.png">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <script src="../assets/js/color-modes.js"></script>
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-YT69KZ91W7"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-YT69KZ91W7');
  </script>
</head>
<body>
  <nav>
    <a href="../" class="logo" aria-label="Home"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16"><path d="M8.354 1.146a.5.5 0 0 0-.708 0l-6 6A.5.5 0 0 0 1.5 7.5v7a.5.5 0 0 0 .5.5h4.5a.5.5 0 0 0 .5-.5v-4h2v4a.5.5 0 0 0 .5.5H14a.5.5 0 0 0 .5-.5v-7a.5.5 0 0 0-.146-.354L13 5.793V2.5a.5.5 0 0 0-.5-.5h-1a.5.5 0 0 0-.5.5v1.293zM2.5 14V7.707l5.5-5.5 5.5 5.5V14H10v-4a.5.5 0 0 0-.5-.5h-3a.5.5 0 0 0-.5.5v4z"/></svg></a>
    <div class="nav-right">
      <div class="nav-links">
        <a href="../about.html">About</a>
        <a href="../blog.html" class="active">Blog</a>
        <a href="../contact.html">Contact</a>
      </div>
      <button class="theme-toggle" aria-label="Toggle theme">
        <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M8 11a3 3 0 1 1 0-6 3 3 0 0 1 0 6m0 1a4 4 0 1 0 0-8 4 4 0 0 0 0 8M8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0m0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13m8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5M3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8m10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0m-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0m9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707M4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708"/>
        </svg>
        <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M6 .278a.77.77 0 0 1 .08.858 7.2 7.2 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277q.792-.001 1.533-.16a.79.79 0 0 1 .81.316.73.73 0 0 1-.031.893A8.35 8.35 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.75.75 0 0 1 6 .278M4.858 1.311A7.27 7.27 0 0 0 1.025 7.71c0 4.02 3.279 7.276 7.319 7.276a7.32 7.32 0 0 0 5.205-2.162q-.506.063-1.029.063c-4.61 0-8.343-3.714-8.343-8.29 0-1.167.242-2.278.681-3.286"/>
          <path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.73 1.73 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.73 1.73 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.73 1.73 0 0 0 1.097-1.097zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.16 1.16 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.16 1.16 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732z"/>
        </svg>
      </button>
    </div>
  </nav>

  <main>
    <a href="../blog.html" class="back-link">&larr; Back to Blog</a>

    <article>
      <h1>Mechanistic Interpretability for Agricultural AI</h1>
      <div class="post-meta">January 5, 2026 &bull; 10 min read</div>

      <p>
        As AI systems become increasingly prevalent in agriculture from automated harvesting to crop monitoring
        understanding <em>how</em> these systems make decisions becomes critical. <span class="glossary-term" data-term="mech-interp">Mechanistic interpretability</span>
        offers a path forward: instead of treating neural networks as <span class="glossary-term" data-term="black-box">black boxes</span>, we can reverse-engineer their
        internal computations to understand what they've actually learned.
      </p>

      <h2>Why Interpretability Matters for Agricultural AI</h2>

      <p>
        Agricultural automation carries real stakes. When a robotic system decides which plants to trim, which
        areas to irrigate, or which produce to harvest, errors can mean lost crops, wasted resources, or
        damaged equipment. Operators need to trust these systems, and trust requires understanding.
      </p>

      <p>
        Consider a vision model trained to detect plant pots in a nursery. The model achieves 95% accuracy
        but what happens with the other 5%? Without interpretability, we can't answer critical questions:
      </p>

      <ul>
        <li>What visual features trigger false positives?</li>
        <li>Does the model rely on <span class="glossary-term" data-term="spurious-correlations">spurious correlations</span> (like pot color) that might not generalize?</li>
        <li>Which conditions cause the model to fail?</li>
        <li>Can we predict when the model will be uncertain?</li>
      </ul>

      <h2>What is Mechanistic Interpretability?</h2>

      <p>
        Mechanistic interpretability is an approach to understanding neural networks by identifying the
        specific computations performed by individual neurons, layers, and circuits. Rather than just
        observing inputs and outputs, we look inside the model to understand its internal representations.
      </p>

      <p>
        Key techniques include:
      </p>

      <ul>
        <li><strong><span class="glossary-term" data-term="feature-viz">Feature visualization</span>:</strong> Generating images that maximally activate specific neurons</li>
        <li><strong>Activation patching:</strong> Measuring how model outputs change when we modify internal activations</li>
        <li><strong>Circuit analysis:</strong> Identifying minimal subnetworks responsible for specific behaviors</li>
        <li><strong>Probing:</strong> Training simple classifiers on intermediate representations to understand what information they encode</li>
      </ul>

      <h2>Applying Interpretability to Vision Models</h2>

      <p>
        In our research, we apply these techniques to vision models like SAM and Faster R-CNN that have been
        adapted for agricultural tasks. Our goal is to answer questions like:
      </p>

      <h3>What do individual channels represent?</h3>

      <p>
        Vision models like SAM have thousands of channels in their feature extractors. By analyzing activation
        patterns, we can identify channels that respond to specific agricultural concepts: pot edges, soil
        texture, plant foliage, shadows, and more. Some channels are "<span class="glossary-term" data-term="monosemantic">monosemantic</span>" responding to a single
        concept while others are "<span class="glossary-term" data-term="polysemantic">polysemantic</span>" activating for multiple, seemingly unrelated features.
      </p>

      <h3>How do representations change during fine-tuning?</h3>

      <p>
        When we fine-tune a foundation model on agricultural data, which representations change? Do we see
        new features emerge, or do existing features get repurposed? Understanding these dynamics helps us
        design more efficient fine-tuning strategies.
      </p>

      <h3>What circuits implement object detection?</h3>

      <p>
        Using activation patching, we can identify the minimal set of channels and connections responsible
        for detecting specific objects. This "circuit extraction" reveals the computational structure the
        model has learned, separate from the vast majority of parameters that may be unused for any given task.
      </p>

      <h2>Practical Benefits</h2>

      <p>
        Interpretability isn't just academic it has practical benefits for deployed systems:
      </p>

      <ul>
        <li><strong>Debugging:</strong> When models fail, interpretability helps identify why and suggests fixes</li>
        <li><strong>Data efficiency:</strong> Understanding what models need to learn guides data collection</li>
        <li><strong>Trust:</strong> Operators are more likely to trust systems they can understand</li>
        <li><strong>Safety:</strong> Identifying failure modes before deployment prevents costly mistakes</li>
      </ul>

      <h2>The Road Ahead</h2>

      <p>
        Mechanistic interpretability for vision models is still in its early days, especially for domain-specific
        applications like agriculture. Much of the foundational work has focused on language models, and adapting
        these techniques to vision requires new methods and tools.
      </p>

      <p>
        Our research aims to bridge this gap by developing interpretability tools specifically for agricultural
        AI systems. By understanding what these models learn, we can build more trustworthy, reliable, and
        effective automation systems for the future of farming.
      </p>

      <blockquote>
        This work draws on the "200 Concrete Problems in Interpretability" framework by Neel Nanda, adapted
        for the unique challenges of vision models in agricultural domains.
      </blockquote>

      <div class="related-post">
        <span class="related-label">Related</span>
        <a href="./circuit-extraction-pot-detection.html">
          <h4>Circuit Extraction: Interpreting Object Detectors</h4>
          <p>Putting mechanistic interpretability into practice&mdash;extracting the minimal computational circuit for pot detection.</p>
          <span class="read-more">Read more &rarr;</span>
        </a>
      </div>

      <div class="related-post">
        <span class="related-label">Related</span>
        <a href="./extracting-vision-features.html">
          <h4>Extracting Features from Vision Model Backbones</h4>
          <p>Technical details on extracting the internal representations needed for interpretability analysis.</p>
          <span class="read-more">Read more &rarr;</span>
        </a>
      </div>
    </article>
  </main>

  <!-- Glossary Modal -->
  <div class="modal-overlay glossary-modal" id="glossary-modal">
    <div class="modal">
      <div class="modal-header">
        <h3 id="glossary-term-title"></h3>
        <button class="modal-close" aria-label="Close">&times;</button>
      </div>
      <div class="modal-body">
        <p id="glossary-term-definition"></p>
        <p class="context-note" id="glossary-term-context"></p>
      </div>
    </div>
  </div>

  <script>
    // Glossary term definitions
    const glossaryTerms = {
      'mech-interp': {
        title: 'Mechanistic Interpretability',
        definition: 'An approach to understanding neural networks by reverse-engineering the specific computations performed by individual components (neurons, layers, circuits). Rather than treating models as black boxes, it seeks to identify what algorithms the model has learned.',
        context: 'We apply mechanistic interpretability to vision models for agriculture, aiming to understand what features they use to detect objects like plant pots—enabling debugging, trust-building, and identification of failure modes.'
      },
      'black-box': {
        title: 'Black Box',
        definition: 'A system whose internal workings are hidden or unknown—you can observe inputs and outputs but not the process in between. Neural networks are often called black boxes because their billions of parameters make it hard to understand how they reach decisions.',
        context: 'Agricultural AI systems carry real stakes, so treating them as black boxes is risky. Mechanistic interpretability opens the box, letting us verify that models use sensible features rather than shortcuts.'
      },
      'spurious-correlations': {
        title: 'Spurious Correlations',
        definition: 'Statistical associations in training data that don\'t reflect true causal relationships. Models can learn to rely on these shortcuts, achieving high accuracy on similar data but failing when the correlation breaks.',
        context: 'A pot detector might learn that pots are always on brown soil—a spurious correlation. When deployed on gravel or concrete, it would fail. Interpretability helps identify such brittle learned associations.'
      },
      'feature-viz': {
        title: 'Feature Visualization',
        definition: 'A technique that generates synthetic images optimized to maximally activate specific neurons or channels. By seeing what "excites" a neuron most, we can understand what visual concept it has learned to detect.',
        context: 'Feature visualization reveals what agricultural concepts individual channels encode—some might detect circular pot edges, others soil texture, others the green of foliage.'
      },
      'monosemantic': {
        title: 'Monosemantic',
        definition: 'A neuron or channel that responds to a single, coherent concept. Monosemantic features are interpretable because their activation has a clear meaning—they fire for one thing and not others.',
        context: 'A monosemantic "pot edge detector" channel would activate only for circular pot boundaries, making it easy to understand and trust. Such channels are the building blocks of interpretable circuits.'
      },
      'polysemantic': {
        title: 'Polysemantic',
        definition: 'A neuron or channel that responds to multiple, seemingly unrelated concepts. Polysemanticity makes interpretation difficult because a single activation could mean several different things.',
        context: 'A polysemantic channel might activate for both pot edges AND tree shadows—unrelated concepts that happen to share some visual features. Disentangling these is an active research challenge.'
      }
    };

    // Modal functionality
    const modal = document.getElementById('glossary-modal');
    const titleEl = document.getElementById('glossary-term-title');
    const definitionEl = document.getElementById('glossary-term-definition');
    const contextEl = document.getElementById('glossary-term-context');
    const closeBtn = modal.querySelector('.modal-close');

    // Open modal on term click
    document.querySelectorAll('.glossary-term').forEach(term => {
      term.addEventListener('click', () => {
        const termId = term.dataset.term;
        const termData = glossaryTerms[termId];

        if (termData) {
          titleEl.textContent = termData.title;
          definitionEl.textContent = termData.definition;
          contextEl.textContent = termData.context;
          modal.classList.add('active');
        }
      });
    });

    // Close modal
    closeBtn.addEventListener('click', () => modal.classList.remove('active'));
    modal.addEventListener('click', (e) => {
      if (e.target === modal) modal.classList.remove('active');
    });
    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape') modal.classList.remove('active');
    });
  </script>

  <footer>
    <div class="social-links">
      <a href="https://linkedin.com/in/nicholasmccarty" target="_blank" rel="noopener" aria-label="LinkedIn">
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401m-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4"/>
        </svg>
      </a>
      <a href="https://github.com/nickmccarty" target="_blank" rel="noopener" aria-label="GitHub">
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8"/>
        </svg>
      </a>
      <a href="mailto:nick@upskilled.consulting" aria-label="Email">
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1zm13 2.383-4.708 2.825L15 11.105zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741M1 11.105l4.708-2.897L1 5.383z"/>
        </svg>
      </a>
    </div>
    <p>&copy; 2025 Nick McCarty. All rights reserved.</p>
  </footer>
</body>
</html>
