<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Training Faster R-CNN for Geospatial Object Detection | Nick McCarty</title>
  <meta name="description" content="A deep dive into training object detection models on aerial imagery, from SAM masks to production-ready Faster R-CNN.">
  <link rel="icon" type="image/png" sizes="32x32" href="../assets/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../assets/images/favicon-16x16.png">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <script src="../assets/js/color-modes.js"></script>
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-YT69KZ91W7"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-YT69KZ91W7');
  </script>
</head>
<body>
  <nav>
    <a href="../" class="logo" aria-label="Home"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16"><path d="M8.354 1.146a.5.5 0 0 0-.708 0l-6 6A.5.5 0 0 0 1.5 7.5v7a.5.5 0 0 0 .5.5h4.5a.5.5 0 0 0 .5-.5v-4h2v4a.5.5 0 0 0 .5.5H14a.5.5 0 0 0 .5-.5v-7a.5.5 0 0 0-.146-.354L13 5.793V2.5a.5.5 0 0 0-.5-.5h-1a.5.5 0 0 0-.5.5v1.293zM2.5 14V7.707l5.5-5.5 5.5 5.5V14H10v-4a.5.5 0 0 0-.5-.5h-3a.5.5 0 0 0-.5.5v4z"/></svg></a>
    <div class="nav-right">
      <div class="nav-links">
        <a href="../about.html">About</a>
        <a href="../blog.html" class="active">Blog</a>
        <a href="../contact.html">Contact</a>
      </div>
      <button class="theme-toggle" aria-label="Toggle theme">
        <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M8 11a3 3 0 1 1 0-6 3 3 0 0 1 0 6m0 1a4 4 0 1 0 0-8 4 4 0 0 0 0 8M8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0m0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13m8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5M3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8m10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0m-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0m9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707M4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708"/>
        </svg>
        <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M6 .278a.77.77 0 0 1 .08.858 7.2 7.2 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277q.792-.001 1.533-.16a.79.79 0 0 1 .81.316.73.73 0 0 1-.031.893A8.35 8.35 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.75.75 0 0 1 6 .278M4.858 1.311A7.27 7.27 0 0 0 1.025 7.71c0 4.02 3.279 7.276 7.319 7.276a7.32 7.32 0 0 0 5.205-2.162q-.506.063-1.029.063c-4.61 0-8.343-3.714-8.343-8.29 0-1.167.242-2.278.681-3.286"/>
          <path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.73 1.73 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.73 1.73 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.73 1.73 0 0 0 1.097-1.097zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.16 1.16 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.16 1.16 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732z"/>
        </svg>
      </button>
    </div>
  </nav>

  <main>
    <a href="../blog.html" class="back-link">&larr; Back to Blog</a>

    <article>
      <h1>Training Faster R-CNN for Geospatial Object Detection</h1>
      <div class="post-meta">December 20, 2025 &bull; 8 min read</div>

      <p>
        Foundation models like SAM are excellent at segmentation, but deploying them at scale on large imagery
        is computationally expensive. A trained object detector runs much faster and can be optimized for your
        specific domain. This post walks through the pipeline I built to train Faster R-CNN models on aerial
        imagery, using SAM's segmentation outputs as training data.
      </p>

      <h2>The Bootstrap Problem</h2>

      <p>
        Training an object detector requires bounding box annotations. For aerial imagery of agricultural
        fields, that means someone manually drawing boxes around thousands of objects&mdash;tedious work
        that doesn't scale. SAM offers an escape hatch: run it once on your imagery to generate high-quality
        polygon masks, then convert those polygons to bounding boxes automatically.
      </p>

      <p>
        The workflow becomes: use SAM (slow but accurate) to <span class="glossary-term" data-term="bootstrap">bootstrap</span> training data, then train a lightweight
        detector (fast at inference) that learns from SAM's outputs. You trade one-time annotation cost for
        a model you can deploy efficiently.
      </p>

      <h2>From Polygons to Bounding Boxes</h2>

      <p>
        SAM outputs polygon masks in GeoJSON format, each with georeferenced coordinates. Converting to
        bounding boxes requires the GeoTIFF's affine transform to map between coordinate reference system
        units and pixel space:
      </p>

      <pre><code>def polygon_to_bbox(polygon, transform):
    minx, miny, maxx, maxy = polygon.bounds
    row_min, col_min = rowcol(transform, minx, maxy)
    row_max, col_max = rowcol(transform, maxx, miny)
    return [col_min, row_min, col_max, row_max]</code></pre>

      <p>
        The key detail: GeoJSON coordinates are typically in a projected CRS (UTM, state plane) where Y
        increases northward, but image coordinates have Y increasing downward. The <code>rowcol</code>
        function from rasterio handles this inversion correctly.
      </p>

      <h2><span class="glossary-term" data-term="hard-negative-mining">Hard Negative Mining</span></h2>

      <p>
        Object detectors learn what to detect and what to ignore. Without explicit negative examples, the
        model might learn spurious correlations&mdash;triggering on shadows, soil patterns, or equipment
        that happens to appear near real objects in the training data.
      </p>

      <p>
        I generate hard negatives by spatially shifting each positive polygon by a fixed distance (1.5 feet
        in CRS units) and using those shifted regions as background examples. The shift is small enough that
        the negative patches look similar to positives&mdash;same lighting, same general context&mdash;but
        don't contain the target object. This forces the model to learn actual object features rather than
        contextual shortcuts.
      </p>

      <pre><code>def generate_hard_negatives(geojson_path, tif_path, shift_distance):
    gdf = gpd.read_file(geojson_path)
    negative_bboxes = []

    for idx, row in gdf.iterrows():
        shifted_poly = translate(row.geometry, xoff=-shift_distance)
        # ... validate and convert to bbox
        negative_bboxes.append(bbox)

    return negative_bboxes</code></pre>

      <h2>Tiling Large Images</h2>

      <p>
        Drone orthomosaics routinely exceed 10,000 pixels per side. Loading a 15,000 x 12,000 pixel image
        directly into GPU memory isn't practical, and resizing destroys the fine detail needed to detect
        small objects.
      </p>

      <p>
        The solution is tile-based training: slice images into 1024x1024 patches with 128 pixel overlap,
        assign bounding boxes to the tiles they intersect, and train on the tiles. The overlap ensures
        objects near tile boundaries appear fully in at least one tile.
      </p>

      <p>
        During tiling, boxes that get clipped at boundaries need filtering. A box that's 90% outside a
        tile isn't useful training signal&mdash;I only keep boxes where the visible portion exceeds a
        minimum size threshold (10 pixels in my configuration).
      </p>

      <h2>Model Architecture</h2>

      <p>
        Faster R-CNN with a ResNet50-FPN backbone hits the sweet spot for this task. The Feature Pyramid
        Network handles objects at multiple scales well, which matters for aerial imagery where the same
        object type can vary significantly in apparent size depending on flight altitude.
      </p>

      <p>
        Starting from <span class="glossary-term" data-term="coco-pretrained">COCO pre-trained weights</span>, I replace only the classification head to match my class
        count (background + one object class). The rest of the network <span class="glossary-term" data-term="transfer-learning">transfers</span> remarkably well despite
        COCO containing nothing like overhead agricultural imagery.
      </p>

      <pre><code>def get_model(num_classes, pretrained=True):
    model = fasterrcnn_resnet50_fpn(pretrained=pretrained)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    return model</code></pre>

      <h2>Training Details</h2>

      <p>
        Data augmentation for aerial imagery differs from natural photos. Rotations and flips are safe&mdash;there's
        no canonical "up" in overhead views. Random 90-degree rotations effectively quadruple the training data.
        Brightness and contrast variations help with lighting differences across flight times.
      </p>

      <p>
        Color jittering, however, tends to hurt. Agricultural objects often have distinctive color signatures
        (dark irrigation circles against green crops), and aggressive color augmentation can destroy that signal.
      </p>

      <p>
        I train with <span class="glossary-term" data-term="sgd">SGD</span>, learning rate 0.005 with <span class="glossary-term" data-term="step-decay">step decay</span> every 10 epochs. Batch size 2 fits comfortably
        on a consumer GPU with 1024x1024 tiles. Training typically converges within 30-50 epochs for datasets
        with a few hundred positive examples.
      </p>

      <h2>Inference and Georeferencing</h2>

      <p>
        At inference time, the same tiling strategy applies: slice the input GeoTIFF, run detection on each
        tile, then map the predicted boxes back to georeferenced coordinates. The affine transform runs in
        reverse&mdash;pixel coordinates to CRS coordinates&mdash;and the output is a GeoJSON file that opens
        directly in QGIS or any GIS tool.
      </p>

      <p>
        Overlapping tiles produce duplicate detections for objects near boundaries. A post-processing step
        merges overlapping boxes, using area-weighted score averaging to combine confidence values:
      </p>

      <pre><code># Find connected components of overlapping detections
for comp in components:
    polys = gdf.loc[comp]
    merged_geom = unary_union(polys.geometry)
    score = np.average(polys.score, weights=polys.geometry.area)
    merged_results.append((merged_geom, score))</code></pre>

      <h2>Results</h2>

      <p>
        A model trained on ~500 positive examples from three orthomosaics generalizes well to new imagery
        from the same sensor and altitude. Inference runs at roughly 50ms per 1024x1024 tile on a RTX 3080&mdash;orders
        of magnitude faster than running SAM on the same patches.
      </p>

      <p>
        The accuracy gap versus SAM is smaller than expected. For well-defined objects with clear boundaries,
        the trained detector achieves 85-90% of SAM's segmentation quality while running 20x faster. The main
        failure modes are unusual lighting conditions and objects partially occluded by vegetation.
      </p>

      <h2>Lessons Learned</h2>

      <p>
        <strong>Quality of bootstrap data matters more than quantity.</strong> Cleaning up SAM's occasional
        false positives before training pays off. A hundred clean examples beat a thousand noisy ones.
      </p>

      <p>
        <strong>Hard negatives are essential.</strong> Without them, the model learns to detect "things near
        field edges" rather than the actual objects. The shifted-polygon approach is simple but effective.
      </p>

      <p>
        <strong>Tile overlap needs tuning.</strong> Too little overlap and you miss boundary objects; too much
        and you waste compute on redundant inference. 128 pixels (12.5% of tile size) works well for objects
        up to ~200 pixels in diameter.
      </p>

      <p>
        <strong>Keep the georeferencing pipeline clean.</strong> Bugs in coordinate transformation are subtle
        and produce results that look almost right but are shifted by a few meters. Validate early with known
        reference points.
      </p>

      <p>
        The full training pipeline is available on <a href="https://github.com/nickmccarty" target="_blank" rel="noopener">GitHub</a>.
      </p>

      <div class="related-post">
        <span class="related-label">Related</span>
        <a href="./fastapi-web-app.html">
          <h4>Building a GeoTIFF Object Detection Web App</h4>
          <p>Deploying this model in a browser-based interface with real-time progress and interactive editing.</p>
          <span class="read-more">Read more &rarr;</span>
        </a>
      </div>

      <div class="related-post">
        <span class="related-label">Related</span>
        <a href="./sam-vs-fasterrcnn-comparison.html">
          <h4>SAM vs Faster R-CNN: A Practical Comparison</h4>
          <p>How does Faster R-CNN compare to SAM for aerial object detection? Speed, accuracy, and architectural trade-offs.</p>
          <span class="read-more">Read more &rarr;</span>
        </a>
      </div>
    </article>
  </main>

  <!-- Glossary Modal -->
  <div class="modal-overlay glossary-modal" id="glossary-modal">
    <div class="modal">
      <div class="modal-header">
        <h3 id="glossary-term-title"></h3>
        <button class="modal-close" aria-label="Close">&times;</button>
      </div>
      <div class="modal-body">
        <p id="glossary-term-definition"></p>
        <p class="context-note" id="glossary-term-context"></p>
      </div>
    </div>
  </div>

  <script>
    // Glossary term definitions
    const glossaryTerms = {
      'bootstrap': {
        title: 'Bootstrap (Data)',
        definition: 'Using an existing model or method to automatically generate training data for a new model. Instead of manual annotation, you leverage one system\'s outputs to train another—trading compute for human labeling effort.',
        context: 'We bootstrap Faster R-CNN training data using SAM\'s segmentation outputs. SAM runs once (slow) to generate polygon masks, which become bounding box annotations for training a fast detector.'
      },
      'hard-negative-mining': {
        title: 'Hard Negative Mining',
        definition: 'Deliberately including difficult negative examples in training—samples that look similar to positives but aren\'t. This forces the model to learn discriminative features rather than superficial correlations.',
        context: 'By shifting positive polygons slightly and using those regions as negatives, we create examples with similar lighting and context but no target object. This prevents the detector from learning shortcuts like "things near field edges."'
      },
      'coco-pretrained': {
        title: 'COCO Pre-trained Weights',
        definition: 'Model weights trained on the COCO (Common Objects in Context) dataset—330K images with 80 object categories. These weights encode general visual features (edges, textures, shapes) that transfer to new domains.',
        context: 'Despite COCO containing no aerial imagery, its pre-trained features transfer surprisingly well. The network has learned to detect edges, shapes, and textures that generalize beyond the original training domain.'
      },
      'transfer-learning': {
        title: 'Transfer Learning',
        definition: 'Using knowledge learned from one task/domain to improve performance on a different but related task. Typically involves starting from pre-trained weights and fine-tuning on the target domain.',
        context: 'We transfer from COCO object detection to aerial pot detection—keeping most network layers frozen and only replacing the classification head. This achieves good results with just hundreds of examples.'
      },
      'sgd': {
        title: 'SGD (Stochastic Gradient Descent)',
        definition: 'An optimization algorithm that updates model weights based on gradients computed from random mini-batches of training data. The "stochastic" (random) sampling adds noise that helps escape local minima.',
        context: 'SGD with momentum is the standard optimizer for Faster R-CNN training. We use learning rate 0.005—higher than typical fine-tuning rates because we\'re adapting to a substantially different domain.'
      },
      'step-decay': {
        title: 'Step Decay',
        definition: 'A learning rate schedule that reduces the learning rate by a fixed factor at predetermined epochs. This allows large updates early in training (exploration) and smaller updates later (fine-tuning).',
        context: 'We decay the learning rate every 10 epochs, typically by 0.1x. This helps the model converge smoothly—large steps initially to find a good region, then smaller steps to settle into the optimum.'
      }
    };

    // Modal functionality
    const modal = document.getElementById('glossary-modal');
    const titleEl = document.getElementById('glossary-term-title');
    const definitionEl = document.getElementById('glossary-term-definition');
    const contextEl = document.getElementById('glossary-term-context');
    const closeBtn = modal.querySelector('.modal-close');

    // Open modal on term click
    document.querySelectorAll('.glossary-term').forEach(term => {
      term.addEventListener('click', () => {
        const termId = term.dataset.term;
        const termData = glossaryTerms[termId];

        if (termData) {
          titleEl.textContent = termData.title;
          definitionEl.textContent = termData.definition;
          contextEl.textContent = termData.context;
          modal.classList.add('active');
        }
      });
    });

    // Close modal
    closeBtn.addEventListener('click', () => modal.classList.remove('active'));
    modal.addEventListener('click', (e) => {
      if (e.target === modal) modal.classList.remove('active');
    });
    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape') modal.classList.remove('active');
    });
  </script>

  <footer>
    <div class="social-links">
      <a href="https://linkedin.com/in/nicholasmccarty" target="_blank" rel="noopener" aria-label="LinkedIn">
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401m-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4"/>
        </svg>
      </a>
      <a href="https://github.com/nickmccarty" target="_blank" rel="noopener" aria-label="GitHub">
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8"/>
        </svg>
      </a>
      <a href="mailto:nick@upskilled.consulting" aria-label="Email">
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 16 16">
          <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1zm13 2.383-4.708 2.825L15 11.105zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741M1 11.105l4.708-2.897L1 5.383z"/>
        </svg>
      </a>
    </div>
    <p>&copy; 2025 Nick McCarty. All rights reserved.</p>
  </footer>
</body>
</html>
